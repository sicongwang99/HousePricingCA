{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Spark CSV Reader\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             Remarks|              rate|\n",
      "+--------------------+------------------+\n",
      "|Plan 4 (1,230 sq....|positiveOrNegative|\n",
      "|This is your chan...|      veryPositive|\n",
      "|Luxurious living ...|      veryPositive|\n",
      "|A new Lennar Moon...|      veryPositive|\n",
      "|BRAND NEW CONSTRU...|      veryPositive|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").load('../compounded.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------+\n",
      "|             Remarks|              rate|length|\n",
      "+--------------------+------------------+------+\n",
      "|Plan 4 (1,230 sq....|positiveOrNegative|   496|\n",
      "|This is your chan...|      veryPositive|  1042|\n",
      "|Luxurious living ...|      veryPositive|  1320|\n",
      "|A new Lennar Moon...|      veryPositive|   715|\n",
      "|BRAND NEW CONSTRU...|      veryPositive|   635|\n",
      "+--------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = df.withColumn('length', length(df['Remarks']))\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "\n",
    "# Create all the features to the data set\n",
    "pos_neg_to_num = StringIndexer(inputCol='rate',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"Remarks\", outputCol=\"token_remarks\")\n",
    "stopremove = StopWordsRemover(inputCol='token_remarks',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"token_remarks\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data)\n",
    "cleaned = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Remarks',\n",
       " 'rate',\n",
       " 'length',\n",
       " 'label',\n",
       " 'token_remarks',\n",
       " 'stop_tokens',\n",
       " 'hash_token',\n",
       " 'idf_token',\n",
       " 'features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262145,[5903,687...|\n",
      "|  0.0|(262145,[3067,395...|\n",
      "|  0.0|(262145,[2548,319...|\n",
      "|  0.0|(262145,[1846,188...|\n",
      "|  0.0|(262145,[1998,618...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show label of ham spame and resulting features\n",
    "cleaned.select(['label', 'features']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|             Remarks|              rate|length|label|       token_remarks|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|!! PRICE REDUCED ...|positiveOrNegative|   359|  1.0|[!!, price, reduc...|[!!, price, reduc...|(262144,[9639,141...|(262144,[9639,141...|(262145,[9639,141...|[-1406.6031222466...|[0.99994627131965...|       0.0|\n",
      "|!!! HUGE PRICE RE...|      veryPositive|   577|  0.0|[!!!, huge, price...|[!!!, huge, price...|(262144,[6872,963...|(262144,[6872,963...|(262145,[6872,963...|[-2236.7724075200...|[1.0,4.3260525616...|       0.0|\n",
      "|!!!!! Big Price R...|      veryPositive|   307|  0.0|[!!!!!, big, pric...|[!!!!!, big, pric...|(262144,[3066,618...|(262144,[3066,618...|(262145,[3066,618...|[-1756.7890122028...|[1.31185229457439...|       1.0|\n",
      "|!!Price Reduction...|      veryPositive|   695|  0.0|[!!price, reducti...|[!!price, reducti...|(262144,[1998,264...|(262144,[1998,264...|(262145,[1998,264...|[-3364.0868178506...|[1.0,8.4740678736...|       0.0|\n",
      "|\"* * PRIME LOCATI...|positiveOrNegative|   509|  1.0|[\"*, *, prime, lo...|[\"*, *, prime, lo...|(262144,[1536,769...|(262144,[1536,769...|(262145,[1536,769...|[-3852.4735830487...|[1.21418231842130...|       1.0|\n",
      "+--------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311584410151937"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
